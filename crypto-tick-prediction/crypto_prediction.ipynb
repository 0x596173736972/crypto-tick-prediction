{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f9bb9c4-f9a5-44ce-9673-d1c40a7260ab",
   "metadata": {},
   "source": [
    "# Crypto Tick-Level Micro-Prediction and Backtest Pipeline\n",
    "# Expert-level implementation for scalping strategy development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2fdd9-ada8-4301-80bb-8771f92ffc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, TimeSeriesSplit\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# SHAP for model explainability\n",
    "import shap\n",
    "\n",
    "# Progress bars and utilities\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Plotting setup\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üì¶ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95ea94f-f146-4e88-b56c-bd0d5acd5f61",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# üìã CONFIGURATION CELL - MODIFY THESE PARAMETERS\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba426be5-5c6e-437e-aa78-874b5f0ecf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Data paths - modify according to your local setup\n",
    "    'DATA_PATH': 'DATA',  # Change this to your actual file path\n",
    "    'OUTPUT_DIR': './results/',\n",
    "    \n",
    "    # Time parameters\n",
    "    'CANDLE_INTERVAL': '1T',  # 1-minute candles\n",
    "    'TIMEZONE': 'UTC',\n",
    "    \n",
    "    # Feature engineering parameters\n",
    "    'IMBALANCE_WINDOW': 10,  # seconds for tick imbalance calculation\n",
    "    'VOLATILITY_WINDOW': 5,  # minutes for rolling volatility\n",
    "    \n",
    "    # ML parameters\n",
    "    'TEST_SIZE': 0.1,  # 10% for testing\n",
    "    'CV_FOLDS': 5,\n",
    "    'RANDOM_STATE': 42,\n",
    "    \n",
    "    # Backtest parameters\n",
    "    'INITIAL_CAPITAL': 10000,  # USD\n",
    "    'POSITION_SIZE': 1000,     # USD per trade\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "Path(CONFIG['OUTPUT_DIR']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration loaded\")\n",
    "print(f\"üìÅ Data path: {CONFIG['DATA_PATH']}\")\n",
    "print(f\"üíæ Output directory: {CONFIG['OUTPUT_DIR']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1fb938-3165-4d17-a5b2-5c10821a20c7",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# üìÅ 1. DATA LOAD & PREPROCESSING\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3dcb63-5ae7-452b-a53f-cc14f09e450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tick_data(file_path):\n",
    "    \"\"\"\n",
    "    Load tick data from csv.gz or parquet file\n",
    "    Expected columns: timestamp, price, amount, side\n",
    "    \"\"\"\n",
    "    print(f\"üì• Loading tick data from: {file_path}\")\n",
    "    \n",
    "    # Determine file type and load accordingly\n",
    "    if file_path.endswith('.parquet'):\n",
    "        df = pd.read_parquet(file_path)\n",
    "    elif file_path.endswith('.csv.gz'):\n",
    "        df = pd.read_csv(file_path, compression='gzip')\n",
    "    elif file_path.endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use .csv, .csv.gz, or .parquet\")\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(df):,} tick records\")\n",
    "    print(f\"üìä Columns: {list(df.columns)}\")\n",
    "    print(f\"üìÖ Date range: {df['timestamp'].min()} to {df['timestamp'].max()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def preprocess_tick_data(df):\n",
    "    \"\"\"\n",
    "    Clean and preprocess tick data\n",
    "    \"\"\"\n",
    "    print(\"üîß Preprocessing tick data...\")\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    if df['timestamp'].dtype == 'object':\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    elif df['timestamp'].dtype in ['int64', 'float64']:\n",
    "        # Assume Unix timestamp in milliseconds\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')\n",
    "    \n",
    "    # Ensure UTC timezone\n",
    "    if df['timestamp'].dt.tz is None:\n",
    "        df['timestamp'] = df['timestamp'].dt.tz_localize('UTC')\n",
    "    else:\n",
    "        df['timestamp'] = df['timestamp'].dt.tz_convert('UTC')\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Convert price and amount to numeric\n",
    "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
    "    df['amount'] = pd.to_numeric(df['amount'], errors='coerce')\n",
    "    \n",
    "    # Clean side column (buy/sell or 1/0)\n",
    "    if df['side'].dtype == 'object':\n",
    "        df['side'] = df['side'].map({'buy': 1, 'sell': 0, 'b': 1, 's': 0})\n",
    "    \n",
    "    # Remove any NaN values\n",
    "    initial_len = len(df)\n",
    "    df = df.dropna()\n",
    "    final_len = len(df)\n",
    "    \n",
    "    if initial_len != final_len:\n",
    "        print(f\"‚ö†Ô∏è  Removed {initial_len - final_len:,} rows with NaN values\")\n",
    "    \n",
    "    print(f\"‚úÖ Preprocessed {len(df):,} tick records\")\n",
    "    return df\n",
    "\n",
    "# For demo purposes, we'll create synthetic data if the file doesn't exist\n",
    "def create_synthetic_data():\n",
    "    \"\"\"Create synthetic tick data for demonstration\"\"\"\n",
    "    print(\"üé≠ Creating synthetic tick data for demonstration...\")\n",
    "    \n",
    "    # Generate 1 day of synthetic tick data\n",
    "    start_time = pd.Timestamp('2024-01-01 00:00:00', tz='UTC')\n",
    "    end_time = start_time + pd.Timedelta(days=1)\n",
    "    \n",
    "    # Base price around 45000 USD\n",
    "    base_price = 45000.0\n",
    "    \n",
    "    # Generate timestamps (random intervals between 10ms to 1000ms)\n",
    "    timestamps = []\n",
    "    current_time = start_time\n",
    "    \n",
    "    while current_time < end_time:\n",
    "        # Random interval between ticks\n",
    "        interval_ms = np.random.exponential(100)  # Average 100ms between ticks\n",
    "        current_time += pd.Timedelta(milliseconds=interval_ms)\n",
    "        timestamps.append(current_time)\n",
    "    \n",
    "    n_ticks = len(timestamps)\n",
    "    print(f\"üìä Generated {n_ticks:,} synthetic ticks\")\n",
    "    \n",
    "    # Generate prices with random walk + some volatility clustering\n",
    "    price_changes = np.random.normal(0, 0.1, n_ticks)  # Small price changes\n",
    "    prices = base_price + np.cumsum(price_changes)\n",
    "    \n",
    "    # Generate amounts (trade sizes)\n",
    "    amounts = np.random.exponential(0.5, n_ticks)  # Exponential distribution for trade sizes\n",
    "    \n",
    "    # Generate sides (buy/sell) with slight bias\n",
    "    sides = np.random.choice([0, 1], n_ticks, p=[0.48, 0.52])  # Slight buy bias\n",
    "    \n",
    "    # Create DataFrame\n",
    "    synthetic_df = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'price': prices,\n",
    "        'amount': amounts,\n",
    "        'side': sides\n",
    "    })\n",
    "    \n",
    "    return synthetic_df\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    if os.path.exists(CONFIG['DATA_PATH']):\n",
    "        tick_data = load_tick_data(CONFIG['DATA_PATH'])\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  File not found: {CONFIG['DATA_PATH']}\")\n",
    "        print(\"üé≠ Using synthetic data for demonstration\")\n",
    "        tick_data = create_synthetic_data()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"üé≠ Using synthetic data for demonstration\")\n",
    "    tick_data = create_synthetic_data()\n",
    "\n",
    "# Preprocess the data\n",
    "tick_data = preprocess_tick_data(tick_data)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nüìà Basic Statistics:\")\n",
    "print(f\"Price range: ${tick_data['price'].min():.2f} - ${tick_data['price'].max():.2f}\")\n",
    "print(f\"Average tick size: ${tick_data['amount'].mean():.4f}\")\n",
    "print(f\"Buy/Sell ratio: {tick_data['side'].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd359500-73d2-4494-bf51-28660f16a004",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# üìä 2. COMPUTE PER-MINUTE HIGH/LOW TICK PATHS\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc320a-1686-4445-9f4c-d9e83044cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_minute_candles(tick_data):\n",
    "    \"\"\"\n",
    "    Resample tick data into 1-minute candles to get P0 (open prices)\n",
    "    \"\"\"\n",
    "    print(\"üïê Computing 1-minute candle opens...\")\n",
    "    \n",
    "    # Set timestamp as index for resampling\n",
    "    df_indexed = tick_data.set_index('timestamp')\n",
    "    \n",
    "    # Resample to 1-minute intervals\n",
    "    candles = df_indexed.groupby(pd.Grouper(freq=CONFIG['CANDLE_INTERVAL'])).agg({\n",
    "        'price': ['first', 'max', 'min', 'last'],\n",
    "        'amount': 'sum',\n",
    "        'side': 'count'\n",
    "    }).dropna()\n",
    "    \n",
    "    # Flatten column names\n",
    "    candles.columns = ['open', 'high', 'low', 'close', 'volume', 'tick_count']\n",
    "    candles = candles.reset_index()\n",
    "    candles.rename(columns={'timestamp': 'minute'}, inplace=True)\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len(candles)} 1-minute candles\")\n",
    "    return candles\n",
    "\n",
    "def compute_tick_paths(tick_data, candles):\n",
    "    \"\"\"\n",
    "    For each 1-minute candle, compute the tick-level path to high and low\n",
    "    \"\"\"\n",
    "    print(\"üõ§Ô∏è  Computing tick paths for each minute...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx, candle in tqdm(candles.iterrows(), total=len(candles), desc=\"Processing candles\"):\n",
    "        minute_start = candle['minute']\n",
    "        minute_end = minute_start + pd.Timedelta(minutes=1)\n",
    "        \n",
    "        # Get ticks for this minute\n",
    "        minute_ticks = tick_data[\n",
    "            (tick_data['timestamp'] >= minute_start) & \n",
    "            (tick_data['timestamp'] < minute_end)\n",
    "        ].copy()\n",
    "        \n",
    "        if len(minute_ticks) == 0:\n",
    "            continue\n",
    "            \n",
    "        P0 = candle['open']  # Open price\n",
    "        high_price = candle['high']\n",
    "        low_price = candle['low']\n",
    "        \n",
    "        # Track when high and low are first reached\n",
    "        high_reached = False\n",
    "        low_reached = False\n",
    "        t_high = None\n",
    "        t_low = None\n",
    "        min_d_high = None\n",
    "        min_d_low = None\n",
    "        \n",
    "        for _, tick in minute_ticks.iterrows():\n",
    "            tick_time = tick['timestamp']\n",
    "            tick_price = tick['price']\n",
    "            \n",
    "            # Check if high is reached for the first time\n",
    "            if not high_reached and tick_price >= high_price:\n",
    "                high_reached = True\n",
    "                t_high = (tick_time - minute_start).total_seconds()\n",
    "                min_d_high = abs(high_price - P0)\n",
    "            \n",
    "            # Check if low is reached for the first time\n",
    "            if not low_reached and tick_price <= low_price:\n",
    "                low_reached = True\n",
    "                t_low = (tick_time - minute_start).total_seconds()\n",
    "                min_d_low = abs(low_price - P0)\n",
    "        \n",
    "        # Determine which came first\n",
    "        up_first = 0  # Default to low first\n",
    "        if high_reached and low_reached:\n",
    "            up_first = 1 if t_high < t_low else 0\n",
    "        elif high_reached and not low_reached:\n",
    "            up_first = 1\n",
    "        elif low_reached and not high_reached:\n",
    "            up_first = 0\n",
    "        else:\n",
    "            # Neither reached (shouldn't happen with proper data)\n",
    "            continue\n",
    "        \n",
    "        # Calculate minimum distances if not already set\n",
    "        if min_d_high is None:\n",
    "            min_d_high = abs(high_price - P0)\n",
    "        if min_d_low is None:\n",
    "            min_d_low = abs(low_price - P0)\n",
    "        \n",
    "        # Set default times if not reached\n",
    "        if t_high is None:\n",
    "            t_high = 60.0  # End of minute\n",
    "        if t_low is None:\n",
    "            t_low = 60.0   # End of minute\n",
    "        \n",
    "        results.append({\n",
    "            'minute': minute_start,\n",
    "            'P0': P0,\n",
    "            'high': high_price,\n",
    "            'low': low_price,\n",
    "            'min_d_high': min_d_high,\n",
    "            'min_d_low': min_d_low,\n",
    "            't_high': t_high,\n",
    "            't_low': t_low,\n",
    "            'up_first': up_first,\n",
    "            'tick_count': len(minute_ticks),\n",
    "            'volume': minute_ticks['amount'].sum()\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(f\"‚úÖ Computed tick paths for {len(results_df)} minutes\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Compute candles and tick paths\n",
    "candles = compute_minute_candles(tick_data)\n",
    "tick_paths = compute_tick_paths(tick_data, candles)\n",
    "\n",
    "# Display statistics\n",
    "print(\"\\nüìä Tick Path Statistics:\")\n",
    "print(f\"Up first ratio: {tick_paths['up_first'].mean():.3f}\")\n",
    "print(f\"Average time to high: {tick_paths['t_high'].mean():.1f}s\")\n",
    "print(f\"Average time to low: {tick_paths['t_low'].mean():.1f}s\")\n",
    "print(f\"Average distance to high: ${tick_paths['min_d_high'].mean():.2f}\")\n",
    "print(f\"Average distance to low: ${tick_paths['min_d_low'].mean():.2f}\")\n",
    "\n",
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribution of min_d_high\n",
    "axes[0, 0].hist(tick_paths['min_d_high'], bins=50, alpha=0.7, color='green')\n",
    "axes[0, 0].set_title('Distribution of Distance to High')\n",
    "axes[0, 0].set_xlabel('Distance (USD)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Distribution of min_d_low\n",
    "axes[0, 1].hist(tick_paths['min_d_low'], bins=50, alpha=0.7, color='red')\n",
    "axes[0, 1].set_title('Distribution of Distance to Low')\n",
    "axes[0, 1].set_xlabel('Distance (USD)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Time to high vs time to low\n",
    "axes[1, 0].scatter(tick_paths['t_high'], tick_paths['t_low'], alpha=0.5)\n",
    "axes[1, 0].set_title('Time to High vs Time to Low')\n",
    "axes[1, 0].set_xlabel('Time to High (seconds)')\n",
    "axes[1, 0].set_ylabel('Time to Low (seconds)')\n",
    "axes[1, 0].plot([0, 60], [0, 60], 'r--', alpha=0.5)\n",
    "\n",
    "# Up first distribution by hour\n",
    "tick_paths['hour'] = tick_paths['minute'].dt.hour\n",
    "hourly_up_first = tick_paths.groupby('hour')['up_first'].mean()\n",
    "axes[1, 1].bar(hourly_up_first.index, hourly_up_first.values, alpha=0.7)\n",
    "axes[1, 1].set_title('Up First Ratio by Hour')\n",
    "axes[1, 1].set_xlabel('Hour of Day')\n",
    "axes[1, 1].set_ylabel('Up First Ratio')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8903f4ec-8762-4ebe-b24d-938e99199521",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# üß† 3. FEATURE ENGINEERING\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703e2220-68e9-4de3-889e-650c8c365bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(tick_data, tick_paths):\n",
    "    \"\"\"\n",
    "    Create features for machine learning model\n",
    "    \"\"\"\n",
    "    print(\"üî¨ Engineering features...\")\n",
    "    \n",
    "    features_df = tick_paths.copy()\n",
    "    \n",
    "    # Time-based features\n",
    "    features_df['hour'] = features_df['minute'].dt.hour\n",
    "    features_df['minute_of_hour'] = features_df['minute'].dt.minute\n",
    "    features_df['day_of_week'] = features_df['minute'].dt.dayofweek\n",
    "    \n",
    "    # Cyclical encoding for time features\n",
    "    features_df['hour_sin'] = np.sin(2 * np.pi * features_df['hour'] / 24)\n",
    "    features_df['hour_cos'] = np.cos(2 * np.pi * features_df['hour'] / 24)\n",
    "    features_df['minute_sin'] = np.sin(2 * np.pi * features_df['minute_of_hour'] / 60)\n",
    "    features_df['minute_cos'] = np.cos(2 * np.pi * features_df['minute_of_hour'] / 60)\n",
    "    features_df['dow_sin'] = np.sin(2 * np.pi * features_df['day_of_week'] / 7)\n",
    "    features_df['dow_cos'] = np.cos(2 * np.pi * features_df['day_of_week'] / 7)\n",
    "    \n",
    "    print(\"‚è∞ Added time-based features\")\n",
    "    \n",
    "    # Tick imbalance features\n",
    "    print(\"‚öñÔ∏è  Computing tick imbalance features...\")\n",
    "    \n",
    "    imbalance_features = []\n",
    "    \n",
    "    for idx, row in tqdm(features_df.iterrows(), total=len(features_df), desc=\"Computing imbalances\"):\n",
    "        minute_start = row['minute']\n",
    "        minute_end = minute_start + pd.Timedelta(minutes=1)\n",
    "        \n",
    "        # Get ticks for this minute\n",
    "        minute_ticks = tick_data[\n",
    "            (tick_data['timestamp'] >= minute_start) & \n",
    "            (tick_data['timestamp'] < minute_end)\n",
    "        ]\n",
    "        \n",
    "        # Compute imbalance in first X seconds\n",
    "        imbalance_end = minute_start + pd.Timedelta(seconds=CONFIG['IMBALANCE_WINDOW'])\n",
    "        early_ticks = minute_ticks[minute_ticks['timestamp'] <= imbalance_end]\n",
    "        \n",
    "        if len(early_ticks) > 0:\n",
    "            buy_ticks = (early_ticks['side'] == 1).sum()\n",
    "            sell_ticks = (early_ticks['side'] == 0).sum()\n",
    "            total_ticks = len(early_ticks)\n",
    "            \n",
    "            imbalance_ratio = (buy_ticks - sell_ticks) / total_ticks if total_ticks > 0 else 0\n",
    "            buy_volume = early_ticks[early_ticks['side'] == 1]['amount'].sum()\n",
    "            sell_volume = early_ticks[early_ticks['side'] == 0]['amount'].sum()\n",
    "            volume_imbalance = (buy_volume - sell_volume) / (buy_volume + sell_volume) if (buy_volume + sell_volume) > 0 else 0\n",
    "        else:\n",
    "            imbalance_ratio = 0\n",
    "            volume_imbalance = 0\n",
    "            total_ticks = 0\n",
    "        \n",
    "        # Average tick size\n",
    "        avg_tick_size = minute_ticks['amount'].mean() if len(minute_ticks) > 0 else 0\n",
    "        \n",
    "        # Price volatility within the minute\n",
    "        price_std = minute_ticks['price'].std() if len(minute_ticks) > 1 else 0\n",
    "        \n",
    "        imbalance_features.append({\n",
    "            'tick_imbalance': imbalance_ratio,\n",
    "            'volume_imbalance': volume_imbalance,\n",
    "            'early_tick_count': total_ticks,\n",
    "            'avg_tick_size': avg_tick_size,\n",
    "            'intra_minute_volatility': price_std\n",
    "        })\n",
    "    \n",
    "    imbalance_df = pd.DataFrame(imbalance_features)\n",
    "    features_df = pd.concat([features_df, imbalance_df], axis=1)\n",
    "    \n",
    "    print(\"‚úÖ Added tick imbalance features\")\n",
    "    \n",
    "    # Rolling features\n",
    "    print(\"üìä Computing rolling features...\")\n",
    "    \n",
    "    # Sort by time for rolling calculations\n",
    "    features_df = features_df.sort_values('minute').reset_index(drop=True)\n",
    "    \n",
    "    # Rolling volatility\n",
    "    features_df['rolling_volatility'] = features_df['intra_minute_volatility'].rolling(\n",
    "        window=CONFIG['VOLATILITY_WINDOW'], min_periods=1\n",
    "    ).mean()\n",
    "    \n",
    "    # Rolling volume\n",
    "    features_df['rolling_volume'] = features_df['volume'].rolling(\n",
    "        window=CONFIG['VOLATILITY_WINDOW'], min_periods=1\n",
    "    ).mean()\n",
    "    \n",
    "    # Rolling tick count\n",
    "    features_df['rolling_tick_count'] = features_df['tick_count'].rolling(\n",
    "        window=CONFIG['VOLATILITY_WINDOW'], min_periods=1\n",
    "    ).mean()\n",
    "    \n",
    "    # Price momentum features\n",
    "    features_df['price_change_1'] = features_df['P0'].pct_change(1)\n",
    "    features_df['price_change_5'] = features_df['P0'].pct_change(5)\n",
    "    \n",
    "    # Relative position within recent range\n",
    "    features_df['price_position'] = (\n",
    "        (features_df['P0'] - features_df['P0'].rolling(10, min_periods=1).min()) /\n",
    "        (features_df['P0'].rolling(10, min_periods=1).max() - features_df['P0'].rolling(10, min_periods=1).min())\n",
    "    ).fillna(0.5)\n",
    "    \n",
    "    print(\"‚úÖ Added rolling features\")\n",
    "    \n",
    "    # Fill any remaining NaN values\n",
    "    features_df = features_df.fillna(0)\n",
    "    \n",
    "    print(f\"üéØ Feature engineering complete. Shape: {features_df.shape}\")\n",
    "    print(f\"üìã Features: {features_df.columns.tolist()}\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# Engineer features\n",
    "features_df = engineer_features(tick_data, tick_paths)\n",
    "\n",
    "# Define feature columns (exclude target and metadata)\n",
    "feature_columns = [\n",
    "    'hour_sin', 'hour_cos', 'minute_sin', 'minute_cos', 'dow_sin', 'dow_cos',\n",
    "    'tick_imbalance', 'volume_imbalance', 'early_tick_count', 'avg_tick_size',\n",
    "    'intra_minute_volatility', 'rolling_volatility', 'rolling_volume',\n",
    "    'rolling_tick_count', 'price_change_1', 'price_change_5', 'price_position',\n",
    "    'min_d_high', 'min_d_low', 't_high', 't_low', 'tick_count', 'volume'\n",
    "]\n",
    "\n",
    "X = features_df[feature_columns]\n",
    "y = features_df['up_first']\n",
    "\n",
    "print(f\"üéØ Features shape: {X.shape}\")\n",
    "print(f\"üè∑Ô∏è  Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b1dab2-181c-4699-9204-6510b81b40ec",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# ü§ñ 4. MACHINE LEARNING CLASSIFIER\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2f6fdd-f99b-41fb-b2aa-86bd7a28e5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost_model(X, y):\n",
    "    \"\"\"\n",
    "    Train XGBoost classifier with time series cross-validation\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Training XGBoost model...\")\n",
    "    \n",
    "    # Time series split (preserving temporal order)\n",
    "    tscv = TimeSeriesSplit(n_splits=CONFIG['CV_FOLDS'])\n",
    "    \n",
    "    # Split data maintaining temporal order\n",
    "    split_idx = int(len(X) * (1 - CONFIG['TEST_SIZE']))\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    \n",
    "    print(f\"üìä Train set: {X_train.shape[0]} samples\")\n",
    "    print(f\"üìä Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    # XGBoost parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "        'min_child_weight': [1, 3, 5]\n",
    "    }\n",
    "    \n",
    "    # Initialize XGBoost classifier\n",
    "    xgb_model = xgb.XGBClassifier(\n",
    "        random_state=CONFIG['RANDOM_STATE'],\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    )\n",
    "    \n",
    "    # Randomized search with time series cross-validation\n",
    "    print(\"üîç Performing hyperparameter tuning...\")\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,  # Number of parameter settings sampled\n",
    "        cv=tscv,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=-1,\n",
    "        random_state=CONFIG['RANDOM_STATE'],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best model\n",
    "    best_model = random_search.best_estimator_\n",
    "    \n",
    "    print(f\"üèÜ Best parameters: {random_search.best_params_}\")\n",
    "    print(f\"üéØ Best CV score: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = best_model.predict(X_train)\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    y_test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    print(\"\\nüìà Model Performance:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Training metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Test metrics\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_precision = precision_score(y_test, y_test_pred)\n",
    "    test_recall = recall_score(y_test, y_test_pred)\n",
    "    test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "    \n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {test_precision:.4f}\")\n",
    "    print(f\"Test Recall: {test_recall:.4f}\")\n",
    "    print(f\"Test AUC: {test_auc:.4f}\")\n",
    "    \n",
    "    print(\"\\nüìã Detailed Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "    \n",
    "    return best_model, X_train, X_test, y_train, y_test, y_test_pred, y_test_proba\n",
    "\n",
    "# Train the model\n",
    "model, X_train, X_test, y_train, y_test, y_test_pred, y_test_proba = train_xgboost_model(X, y)\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüîù Top 10 Most Important Features:\")\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_features = feature_importance.head(15)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 15 Feature Importance (XGBoost)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f5711-8aae-4ebd-920d-b87105c0b83e",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# üìà 5. SHAP ANALYSIS\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565167b6-8e11-4f7e-98ef-a0f32ece8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_shap_analysis(model, X_train, X_test, feature_columns):\n",
    "    \"\"\"\n",
    "    Perform SHAP analysis for model explainability\n",
    "    \"\"\"\n",
    "    print(\"üîç Performing SHAP analysis...\")\n",
    "    \n",
    "    # Create SHAP explainer\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    \n",
    "    # Calculate SHAP values for test set (sample if too large)\n",
    "    sample_size = min(1000, len(X_test))\n",
    "    X_sample = X_test.sample(n=sample_size, random_state=CONFIG['RANDOM_STATE'])\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    \n",
    "    print(f\"üìä SHAP values calculated for {sample_size} samples\")\n",
    "    \n",
    "    # SHAP summary plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=feature_columns, show=False)\n",
    "    plt.title('SHAP Summary Plot - Feature Impact on Predictions')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # SHAP feature importance\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values, X_sample, feature_names=feature_columns, plot_type=\"bar\", show=False)\n",
    "    plt.title('SHAP Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Feature interaction analysis (top features only)\n",
    "    top_5_features = feature_importance.head(5)['feature'].tolist()\n",
    "    top_5_indices = [feature_columns.index(feat) for feat in top_5_features]\n",
    "    \n",
    "    print(f\"üîó Analyzing interactions for top 5 features: {top_5_features}\")\n",
    "    \n",
    "    for i, feat_idx in enumerate(top_5_indices[:3]):  # Show top 3 to avoid clutter\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        shap.dependence_plot(feat_idx, shap_values, X_sample, feature_names=feature_columns, show=False)\n",
    "        plt.title(f'SHAP Dependence Plot - {feature_columns[feat_idx]}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return shap_values, explainer\n",
    "\n",
    "# Perform SHAP analysis\n",
    "shap_values, explainer = perform_shap_analysis(model, X_train, X_test, feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62ee804-d566-419d-8805-40572fc68d3d",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# üí∞ 6. BACKTEST STRATEGY\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacaed3-0638-4c66-b63d-dd5e5c0dd60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_strategy(features_df, model, X):\n",
    "    \"\"\"\n",
    "    Backtest the trading strategy\n",
    "    \"\"\"\n",
    "    print(\"üí∞ Running backtest...\")\n",
    "    \n",
    "    # Create backtest dataframe\n",
    "    backtest_df = features_df.copy()\n",
    "    backtest_df['predicted_up_first'] = model.predict(X)\n",
    "    backtest_df['prediction_proba'] = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Initialize backtest variables\n",
    "    initial_capital = CONFIG['INITIAL_CAPITAL']\n",
    "    position_size = CONFIG['POSITION_SIZE']\n",
    "    \n",
    "    backtest_df['position'] = 0  # 1 for long, -1 for short, 0 for no position\n",
    "    backtest_df['entry_price'] = 0.0\n",
    "    backtest_df['tp_level'] = 0.0\n",
    "    backtest_df['sl_level'] = 0.0\n",
    "    backtest_df['trade_pnl'] = 0.0\n",
    "    backtest_df['cumulative_pnl'] = 0.0\n",
    "    backtest_df['trade_outcome'] = ''  # 'TP', 'SL', or 'None'\n",
    "    \n",
    "    # Track portfolio metrics\n",
    "    total_trades = 0\n",
    "    winning_trades = 0\n",
    "    losing_trades = 0\n",
    "    cumulative_pnl = 0.0\n",
    "    max_drawdown = 0.0\n",
    "    peak_pnl = 0.0\n",
    "    \n",
    "    print(\"üîÑ Simulating trades...\")\n",
    "    \n",
    "    for idx in tqdm(range(len(backtest_df)), desc=\"Backtesting\"):\n",
    "        row = backtest_df.iloc[idx]\n",
    "        \n",
    "        # Skip if insufficient historical data for prediction\n",
    "        if idx < CONFIG['VOLATILITY_WINDOW']:\n",
    "            continue\n",
    "        \n",
    "        # Get prediction and actual outcome\n",
    "        predicted_up_first = row['predicted_up_first']\n",
    "        actual_up_first = row['up_first']\n",
    "        prediction_confidence = row['prediction_proba']\n",
    "        \n",
    "        # Only trade if confidence is above threshold (optional filter)\n",
    "        confidence_threshold = 0.55  # Trade only if confidence > 55%\n",
    "        if abs(prediction_confidence - 0.5) < (confidence_threshold - 0.5):\n",
    "            continue\n",
    "        \n",
    "        # Entry price (open of the minute)\n",
    "        entry_price = row['P0']\n",
    "        \n",
    "        # Set position based on prediction\n",
    "        if predicted_up_first == 1:\n",
    "            # Predict high will be reached first - go long\n",
    "            position = 1\n",
    "            tp_level = entry_price + row['min_d_high']\n",
    "            sl_level = entry_price - row['min_d_low']\n",
    "        else:\n",
    "            # Predict low will be reached first - go short\n",
    "            position = -1\n",
    "            tp_level = entry_price - row['min_d_low']\n",
    "            sl_level = entry_price + row['min_d_high']\n",
    "        \n",
    "        # Determine trade outcome based on actual market behavior\n",
    "        if actual_up_first == 1:\n",
    "            # High was reached first\n",
    "            if position == 1:\n",
    "                # Long position - TP hit\n",
    "                trade_pnl = row['min_d_high'] * (position_size / entry_price)\n",
    "                trade_outcome = 'TP'\n",
    "            else:\n",
    "                # Short position - SL hit\n",
    "                trade_pnl = -row['min_d_high'] * (position_size / entry_price)\n",
    "                trade_outcome = 'SL'\n",
    "        else:\n",
    "            # Low was reached first\n",
    "            if position == -1:\n",
    "                # Short position - TP hit\n",
    "                trade_pnl = row['min_d_low'] * (position_size / entry_price)\n",
    "                trade_outcome = 'TP'\n",
    "            else:\n",
    "                # Long position - SL hit\n",
    "                trade_pnl = -row['min_d_low'] * (position_size / entry_price)\n",
    "                trade_outcome = 'SL'\n",
    "        \n",
    "        # Update backtest dataframe\n",
    "        backtest_df.loc[idx, 'position'] = position\n",
    "        backtest_df.loc[idx, 'entry_price'] = entry_price\n",
    "        backtest_df.loc[idx, 'tp_level'] = tp_level\n",
    "        backtest_df.loc[idx, 'sl_level'] = sl_level\n",
    "        backtest_df.loc[idx, 'trade_pnl'] = trade_pnl\n",
    "        backtest_df.loc[idx, 'trade_outcome'] = trade_outcome\n",
    "        \n",
    "        # Update cumulative metrics\n",
    "        cumulative_pnl += trade_pnl\n",
    "        backtest_df.loc[idx, 'cumulative_pnl'] = cumulative_pnl\n",
    "        \n",
    "        # Track trade statistics\n",
    "        total_trades += 1\n",
    "        if trade_pnl > 0:\n",
    "            winning_trades += 1\n",
    "        else:\n",
    "            losing_trades += 1\n",
    "        \n",
    "        # Update drawdown tracking\n",
    "        if cumulative_pnl > peak_pnl:\n",
    "            peak_pnl = cumulative_pnl\n",
    "        \n",
    "        current_drawdown = peak_pnl - cumulative_pnl\n",
    "        if current_drawdown > max_drawdown:\n",
    "            max_drawdown = current_drawdown\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    win_rate = winning_trades / total_trades if total_trades > 0 else 0\n",
    "    avg_win = backtest_df[backtest_df['trade_pnl'] > 0]['trade_pnl'].mean()\n",
    "    avg_loss = backtest_df[backtest_df['trade_pnl'] < 0]['trade_pnl'].mean()\n",
    "    profit_factor = abs(avg_win * winning_trades / (avg_loss * losing_trades)) if losing_trades > 0 and avg_loss != 0 else float('inf')\n",
    "    \n",
    "    # Sharpe ratio (simplified - assuming daily returns)\n",
    "    returns = backtest_df['trade_pnl'].dropna()\n",
    "    sharpe_ratio = returns.mean() / returns.std() * np.sqrt(252) if returns.std() > 0 else 0\n",
    "    \n",
    "    print(\"\\nüìä Backtest Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total Trades: {total_trades:,}\")\n",
    "    print(f\"Winning Trades: {winning_trades:,}\")\n",
    "    print(f\"Losing Trades: {losing_trades:,}\")\n",
    "    print(f\"Win Rate: {win_rate:.2%}\")\n",
    "    print(f\"Total PnL: ${cumulative_pnl:.2f}\")\n",
    "    print(f\"Max Drawdown: ${max_drawdown:.2f}\")\n",
    "    print(f\"Average Win: ${avg_win:.2f}\" if not pd.isna(avg_win) else \"Average Win: N/A\")\n",
    "    print(f\"Average Loss: ${avg_loss:.2f}\" if not pd.isna(avg_loss) else \"Average Loss: N/A\")\n",
    "    print(f\"Profit Factor: {profit_factor:.2f}\" if profit_factor != float('inf') else \"Profit Factor: ‚àû\")\n",
    "    print(f\"Sharpe Ratio: {sharpe_ratio:.2f}\")\n",
    "    print(f\"Return on Capital: {(cumulative_pnl / initial_capital) * 100:.2f}%\")\n",
    "    \n",
    "    return backtest_df, {\n",
    "        'total_trades': total_trades,\n",
    "        'winning_trades': winning_trades,\n",
    "        'losing_trades': losing_trades,\n",
    "        'win_rate': win_rate,\n",
    "        'total_pnl': cumulative_pnl,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'avg_win': avg_win,\n",
    "        'avg_loss': avg_loss,\n",
    "        'profit_factor': profit_factor,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'return_on_capital': (cumulative_pnl / initial_capital) * 100\n",
    "    }\n",
    "\n",
    "# Run backtest\n",
    "backtest_results, metrics = backtest_strategy(features_df, model, X)\n",
    "\n",
    "# Plot backtest results\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 15))\n",
    "\n",
    "# 1. Cumulative PnL curve\n",
    "traded_results = backtest_results[backtest_results['trade_pnl'] != 0].copy()\n",
    "if len(traded_results) > 0:\n",
    "    axes[0, 0].plot(traded_results['minute'], traded_results['cumulative_pnl'], linewidth=2)\n",
    "    axes[0, 0].set_title('Cumulative PnL Over Time')\n",
    "    axes[0, 0].set_xlabel('Time')\n",
    "    axes[0, 0].set_ylabel('Cumulative PnL ($)')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 2. Trade PnL distribution\n",
    "trade_pnls = traded_results['trade_pnl'].dropna()\n",
    "if len(trade_pnls) > 0:\n",
    "    axes[0, 1].hist(trade_pnls, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 1].axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "    axes[0, 1].set_title('Distribution of Trade PnL')\n",
    "    axes[0, 1].set_xlabel('Trade PnL ($)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Win/Loss by prediction confidence\n",
    "if len(traded_results) > 0:\n",
    "    win_trades = traded_results[traded_results['trade_pnl'] > 0]\n",
    "    loss_trades = traded_results[traded_results['trade_pnl'] <= 0]\n",
    "    \n",
    "    axes[1, 0].scatter(win_trades['prediction_proba'], win_trades['trade_pnl'], \n",
    "                      color='green', alpha=0.6, label='Wins', s=20)\n",
    "    axes[1, 0].scatter(loss_trades['prediction_proba'], loss_trades['trade_pnl'], \n",
    "                      color='red', alpha=0.6, label='Losses', s=20)\n",
    "    axes[1, 0].set_title('Trade Outcome vs Prediction Confidence')\n",
    "    axes[1, 0].set_xlabel('Prediction Probability')\n",
    "    axes[1, 0].set_ylabel('Trade PnL ($)')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 4. Monthly returns heatmap (if we have enough data)\n",
    "if len(traded_results) > 0:\n",
    "    traded_results['year_month'] = traded_results['minute'].dt.to_period('M')\n",
    "    monthly_returns = traded_results.groupby('year_month')['trade_pnl'].sum()\n",
    "    \n",
    "    if len(monthly_returns) > 1:\n",
    "        monthly_df = monthly_returns.reset_index()\n",
    "        monthly_df['year'] = monthly_df['year_month'].dt.year\n",
    "        monthly_df['month'] = monthly_df['year_month'].dt.month\n",
    "        \n",
    "        pivot_table = monthly_df.pivot(index='year', columns='month', values='trade_pnl')\n",
    "        \n",
    "        sns.heatmap(pivot_table, annot=True, cmap='RdYlGn', center=0, \n",
    "                   fmt='.0f', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Monthly Returns Heatmap')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'Insufficient data\\nfor monthly analysis', \n",
    "                       ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Monthly Returns Heatmap')\n",
    "\n",
    "# 5. Drawdown curve\n",
    "if len(traded_results) > 0:\n",
    "    traded_results['running_max'] = traded_results['cumulative_pnl'].expanding().max()\n",
    "    traded_results['drawdown'] = traded_results['cumulative_pnl'] - traded_results['running_max']\n",
    "    \n",
    "    axes[2, 0].fill_between(traded_results['minute'], traded_results['drawdown'], 0, \n",
    "                           color='red', alpha=0.3)\n",
    "    axes[2, 0].plot(traded_results['minute'], traded_results['drawdown'], color='red', linewidth=1)\n",
    "    axes[2, 0].set_title('Drawdown Over Time')\n",
    "    axes[2, 0].set_xlabel('Time')\n",
    "    axes[2, 0].set_ylabel('Drawdown ($)')\n",
    "    axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Rolling win rate\n",
    "if len(traded_results) > 0:\n",
    "    window_size = max(50, len(traded_results) // 20)  # Adaptive window size\n",
    "    traded_results['rolling_wins'] = (traded_results['trade_pnl'] > 0).rolling(\n",
    "        window=window_size, min_periods=10\n",
    "    ).mean()\n",
    "    \n",
    "    axes[2, 1].plot(traded_results['minute'], traded_results['rolling_wins'], linewidth=2)\n",
    "    axes[2, 1].axhline(y=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[2, 1].set_title(f'Rolling Win Rate ({window_size} trades)')\n",
    "    axes[2, 1].set_xlabel('Time')\n",
    "    axes[2, 1].set_ylabel('Win Rate')\n",
    "    axes[2, 1].set_ylim(0, 1)\n",
    "    axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Trade analysis by hour of day\n",
    "if len(traded_results) > 0:\n",
    "    hourly_analysis = traded_results.groupby(traded_results['minute'].dt.hour).agg({\n",
    "        'trade_pnl': ['count', 'sum', 'mean'],\n",
    "        'prediction_proba': 'mean'\n",
    "    }).round(3)\n",
    "    \n",
    "    hourly_analysis.columns = ['Trade_Count', 'Total_PnL', 'Avg_PnL', 'Avg_Confidence']\n",
    "    hourly_analysis['Win_Rate'] = traded_results.groupby(\n",
    "        traded_results['minute'].dt.hour\n",
    "    ).apply(lambda x: (x['trade_pnl'] > 0).mean()).round(3)\n",
    "    \n",
    "    print(\"\\nüïê Performance by Hour of Day:\")\n",
    "    print(hourly_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0571b58-1262-4ffb-8fe7-20b535dc1dcc",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# üíæ 7. EXPORT & SAVE RESULTS\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49680c3c-9393-449a-80b3-ddf4babc90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(backtest_results, metrics, model, feature_importance):\n",
    "    \"\"\"\n",
    "    Save all results to disk\n",
    "    \"\"\"\n",
    "    print(\"üíæ Saving results to disk...\")\n",
    "    \n",
    "    output_dir = Path(CONFIG['OUTPUT_DIR'])\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save backtest results\n",
    "    backtest_file = output_dir / f\"backtest_results_{timestamp}.parquet\"\n",
    "    backtest_results.to_parquet(backtest_file)\n",
    "    print(f\"üìä Backtest results saved to: {backtest_file}\")\n",
    "    \n",
    "    # Save model metrics\n",
    "    metrics_file = output_dir / f\"model_metrics_{timestamp}.json\"\n",
    "    import json\n",
    "    \n",
    "    # Convert numpy types to Python types for JSON serialization\n",
    "    metrics_json = {}\n",
    "    for key, value in metrics.items():\n",
    "        if isinstance(value, (np.integer, np.floating)):\n",
    "            metrics_json[key] = float(value)\n",
    "        elif pd.isna(value):\n",
    "            metrics_json[key] = None\n",
    "        else:\n",
    "            metrics_json[key] = value\n",
    "    \n",
    "    with open(metrics_file, 'w') as f:\n",
    "        json.dump(metrics_json, f, indent=2)\n",
    "    print(f\"üìà Model metrics saved to: {metrics_file}\")\n",
    "    \n",
    "    # Save feature importance\n",
    "    feature_importance_file = output_dir / f\"feature_importance_{timestamp}.csv\"\n",
    "    feature_importance.to_csv(feature_importance_file, index=False)\n",
    "    print(f\"üîù Feature importance saved to: {feature_importance_file}\")\n",
    "    \n",
    "    # Save model\n",
    "    model_file = output_dir / f\"xgboost_model_{timestamp}.json\"\n",
    "    model.save_model(str(model_file))\n",
    "    print(f\"ü§ñ Model saved to: {model_file}\")\n",
    "    \n",
    "    # Create summary report\n",
    "    summary_file = output_dir / f\"trading_strategy_summary_{timestamp}.txt\"\n",
    "    \n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(\"BTC/USDT Tick-Level Micro-Prediction Strategy Report\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Data period: {backtest_results['minute'].min()} to {backtest_results['minute'].max()}\\n\\n\")\n",
    "        \n",
    "        f.write(\"BACKTEST PERFORMANCE SUMMARY:\\n\")\n",
    "        f.write(\"-\" * 30 + \"\\n\")\n",
    "        f.write(f\"Total Trades: {metrics['total_trades']:,}\\n\")\n",
    "        f.write(f\"Win Rate: {metrics['win_rate']:.2%}\\n\")\n",
    "        f.write(f\"Total PnL: ${metrics['total_pnl']:.2f}\\n\")\n",
    "        f.write(f\"Max Drawdown: ${metrics['max_drawdown']:.2f}\\n\")\n",
    "        f.write(f\"Sharpe Ratio: {metrics['sharpe_ratio']:.2f}\\n\")\n",
    "        f.write(f\"Return on Capital: {metrics['return_on_capital']:.2f}%\\n\\n\")\n",
    "        \n",
    "        f.write(\"TOP 10 MOST IMPORTANT FEATURES:\\n\")\n",
    "        f.write(\"-\" * 35 + \"\\n\")\n",
    "        for idx, row in feature_importance.head(10).iterrows():\n",
    "            f.write(f\"{row['feature']}: {row['importance']:.4f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nSTRATEGY CONFIGURATION:\\n\")\n",
    "        f.write(\"-\" * 25 + \"\\n\")\n",
    "        for key, value in CONFIG.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    print(f\"üìã Summary report saved to: {summary_file}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ All results saved successfully!\")\n",
    "    \n",
    "    return {\n",
    "        'backtest_file': backtest_file,\n",
    "        'metrics_file': metrics_file,\n",
    "        'feature_importance_file': feature_importance_file,\n",
    "        'model_file': model_file,\n",
    "        'summary_file': summary_file\n",
    "    }\n",
    "\n",
    "# Save all results\n",
    "saved_files = save_results(backtest_results, metrics, model, feature_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e914108-b740-4ae3-bfdc-327993f26d3f",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# üéØ FINAL SUMMARY AND RECOMMENDATIONS\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0637b6e-a3a4-4715-8ea2-05b5aeee04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ STRATEGY ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE OVERVIEW:\")\n",
    "print(f\"   ‚Ä¢ Processed {len(tick_data):,} ticks into {len(features_df):,} 1-minute candles\")\n",
    "print(f\"   ‚Ä¢ Trained model on {len(X_train):,} samples, tested on {len(X_test):,} samples\")\n",
    "print(f\"   ‚Ä¢ Executed {metrics['total_trades']:,} trades with {metrics['win_rate']:.1%} win rate\")\n",
    "print(f\"   ‚Ä¢ Generated ${metrics['total_pnl']:.2f} total PnL ({metrics['return_on_capital']:.1f}% return)\")\n",
    "\n",
    "print(f\"\\nüîë KEY INSIGHTS:\")\n",
    "most_important_feature = feature_importance.iloc[0]['feature']\n",
    "print(f\"   ‚Ä¢ Most predictive feature: {most_important_feature}\")\n",
    "print(f\"   ‚Ä¢ Model AUC: {roc_auc_score(y_test, y_test_proba):.3f}\")\n",
    "\n",
    "if metrics['total_trades'] > 0:\n",
    "    avg_trade_duration = \"~1 minute\"  # Since we trade every minute\n",
    "    print(f\"   ‚Ä¢ Average trade duration: {avg_trade_duration}\")\n",
    "    \n",
    "    if metrics['win_rate'] > 0.5:\n",
    "        print(\"   ‚Ä¢ ‚úÖ Strategy shows positive edge\")\n",
    "    else:\n",
    "        print(\"   ‚Ä¢ ‚ö†Ô∏è  Strategy shows negative edge - consider refinement\")\n",
    "\n",
    "print(f\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"   ‚Ä¢ Test on out-of-sample data from different time periods\")\n",
    "print(\"   ‚Ä¢ Implement position sizing based on prediction confidence\")\n",
    "print(\"   ‚Ä¢ Add transaction costs and slippage modeling\")\n",
    "print(\"   ‚Ä¢ Consider ensemble methods or alternative algorithms\")\n",
    "print(\"   ‚Ä¢ Implement live trading with proper risk management\")\n",
    "\n",
    "print(f\"\\nüìÅ All results saved to: {CONFIG['OUTPUT_DIR']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üèÅ PIPELINE EXECUTION COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
